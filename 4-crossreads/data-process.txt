
1) DONE Get the list of diaries 100% transcribed 
    INPUT -> 8 pages from the API of SLNSW
        -> for XXX from 0 to 8:
        -> get: http://transcripts.sl.nsw.gov.au/api/entity_node/?fields=nid,title,url,percentage_unlocked,record_number&parameters[collection]=4&parameters[type]=transcipt_document&page=XXX&pagesize=500&sort=percentage_unlocked&direction=ASC
        -> Merge these 8 files in 1. In JSON format.
    OUTPUT -> new file: data-diaries-list.json

2) DONE Get pages for each diary
    INPUT data-diaries-list.json
        run 2-getPagesList-forEachDiary.py
        -> Download all the diary details
        -> save each one in XML-diaries-list.xml/ as .json file
    OUTPUT files in 2-PagesList-forEachDiary/*.json

????) from data-diaries-list.json to diaries.json

3) DONE Download Transcription
    INPUT files in 2-PagesList-forEachDiary/*.json
        -> for each file: download page info and text transcription
        -> save it .txt
    OUTPUT 80772 transcriptions as txt files in 3-transcriptions/[diary_nid]/[page_nid].txt

3bis) DONE Download all images for each page
    INPUT: 2-PagesList-forEachDiary/[Diary-id].json
        -> run  python  3-Download-image-PerDiary-PerPage.py
        -> if page was not found: check for the default image, initialy set to "img/parts-of-the-leg.jpg"
    OUTPUT jpg files one per each page in 3-Transcriptions/[Diary-id]/[page-id].jpg

4) DONE I found API inconsistencies: 
    file        2-PagesList-forEachDiary/325182.json (an 93 other files)
    and file    2-PagesList-forEachDiary/182219.json (and 668 other files)
    have different JSON structure (!)
    -> added an "elif" for the two cases and empty diary detection
    -> Empty diaries detected: 115204.json 100411.json 115243.json 365805.json 100394.json 115202.json 338054.json 100393.json 100407.json 115213.json 115109.json 318442.json 115164.json 115175.json 100365.json 115212.json 281689.json 115162.json 318465.json 115245.json. 308755.json 100350.json 100403.json 325204.json  
    -> I remove all those entries
    -> Diaries now: 780
    -> API inconsitency inthis this file: http://transcripts.sl.nsw.gov.au/api/entity_node/272772

5) Parse diaries list and extend  diary metadata
    INPUT: "diaries-simple.json"
        run 5-diaries-title-parser.py > diaries.json
        Added fields: author', 'date_end', 'date_start', 'kind', 'nid', 'title', 'url'.
        PENDING of a manual review
    OUTPUT diaries.json
        -> manual edition of diaries.json. Tasks done:
            * Correct not well parsed data
            * As a first approach (later can be easily converted to general categories), I did chosse broad names for the propierty 'kind'.  For example: "letters from Gallipolli", instead of just "letters"
            * Format has been converted to correct JSON.
            * Notes on using the data:
                > if date_start is "":
                    if date_end is [only a year]:
                        date_start = date_end #put a year long diary 

6) DONE Topic model analysis with Mallet
    INPUT: all pages 3-TranscriptionsCLEAN/*
        > run mallet: check Mallet-commands.txt
        > Analysis done: 30, 50 and 100 topics. 
        > Human labelling has been tested for 30, 50 and 100 topics. 100 topics includes more and more specific topics. 
    OUTPUT: 6-Mallet-results/topics50:   crossreads_compostion.txt crossreads_keys.txt [converted to csv)

7) DONE Browser for topic models pages. Get n pages from topic model i, sort descendint or ascending.
    INPUT: 6-Mallet-results/topics50/crossreads_compostion.cvs'
        -> run 7-TopicMode-CSVbrowser.py [TM id] [Num pages] [desc/asc]
    OUTPUT (in console) text of [num pages] sorted by [TM id]. [desc/asc]/ This output is thrown to a files for each 
     TM, in version desc and asc. These will be precalculated files offered later in the interface. 

8) Check empty pages, and decide what to do 

9) DONE Review labelling using:
    INPUT: Mallet-results/topics100/crossreads_(  AND  7-TopicMode-CSVbrowser.py [TM id] [Num pages] [desc/asc] 
        * first labelling draft, supported by TopicMode-CSVbrowser
    OUTPUT: 6-Mallet-results/crossreads4_keys.cvs

9.1) Topic labels:
    INPUT 6-Mallet-results/crossreads4_keys.cvs
        * Label/sublabel TMs, supported by TopicMode-CSVbrowser
    OUTPUT: 6-Mallet-results/crossreads4_keys-LABELS.cvs


9.3) 
    INPUT: 6-Mallet-results/crossreads4_keys-LABELS.cvs
        - Group same-label TMs. And decide
        - Add TopicGroups IDs
    OUTPUT: Create final topic models json file: topics.json

10) Topic models (TM) stats:
    - Number of pages with first TM / TM
    DONE - Score average of first TM
    - Gauss (campana) for first TM score

        R-studio:

        > Imported crossreads4_composition.csv as cc
        > # from http://stackoverflow.com/questions/12864867/row-based-summary-calculations
        > Stats <- function(x){
            #Mean <- sapply(x, mean, na.rm=TRUE)
            SD <- sd(x, na.rm=TRUE)
            Min <- min(x, na.rm=TRUE)
            Max <- max(x, na.rm=TRUE)
            return(c(SD=SD, Min=Min, Max=Max))
          }
        > newcc <- cc[,sapply(cc, is.numeric)]
        > onlycc <- newcc[,-1]
        > myStats <- cbind(onlycc, t(apply(onlycc,1, Stats))) # table with TM score + 
        > onlyStats <- myStats[, c("SD", "Min","Max")]
        > sapply(onlyStats["Max"], mean)
              Max 
        > 0.3403345 #Score average of first TM

        PLOT
        > y <- 1/sqrt(2*pi)*exp(-x^2/2)
        > x <- onlyStats[, c("Max")]
        > plot(x,y,type="l", col="blue", main="Max TM score distribution")

        HISTOGRAM
        > g <- onlyStats[,3]
        > h<-hist(g, breaks=50, density=10, col="blue", xlab="Accuracy", main="Overall") 
        > xfit<-seq(min(g),max(g),length=40) 
        > yfit<-dnorm(xfit,mean=mean(g),sd=sd(g)) 
        > yfit <- yfit*diff(h$mids[1:2])*length(g) 
        > lines(xfit, yfit, col="black", lwd=2)



11) Generate final files for each diary with pageID:[TM scores descending]
    (OR all together in one file for all diaries, as in crossreads version3?? I don't think so) 


##############################################################################

Data ToDo:

Diaries.json:
DONE    > clear it up with diaries in ls 3-Transciption
    > (Optional) Add topic_line: [TMis]:[score], ...
DONE    > transfor "data" in three subfields: "day", "Month", "year"
    
DONE Topics: topics.json
    DONE > decide topic groups (and TGids), and topics for each group (using Mallet TMids)    

DONE terms.json <- converted from crossreads4_keys.csv


[diaryID].json
    > build each file reading crossreads4_composition.txt
    > 

        
Subsets: 
    > define format for subsets of pages. Probably use [diaryID].json format
    > Generate topics for 
    > 


###############################################################################




-------------------------------
NOTE: These documents are in the collection but are not related to WWI-Diaries. These are not included in the visualization:

http://transcripts.sl.nsw.gov.au/api/entity_node/369991/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369992/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369993/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369994/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369995/?fields=nid,title,record_number,percentage_unlocked,transcript_pages

