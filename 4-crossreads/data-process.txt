
1) DONE Get the list of diaries 100% transcribed 
    INPUT -> 8 pages from the API of SLNSW
        -> for XXX from 0 to 8:
        -> get: http://transcripts.sl.nsw.gov.au/api/entity_node/?fields=nid,title,url,percentage_unlocked,record_number&parameters[collection]=4&parameters[type]=transcipt_document&page=XXX&pagesize=500&sort=percentage_unlocked&direction=ASC
        -> Merge these 8 files in 1. In JSON format.
    OUTPUT -> new file: data-diaries-list.json

2) DONE Get pages for each diary
    INPUT data-diaries-list.json
        run 2-getPagesList-forEachDiary.py
        -> Download all the diary details
        -> save each one in XML-diaries-list.xml/ as .json file
    OUTPUT files in 2-PagesList-forEachDiary/*.json

????) from data-diaries-list.json to diaries.json

3) DONE Download Transcription
    INPUT files in 2-PagesList-forEachDiary/*.json
        -> for each file: download page info and text transcription
        -> save it .txt
    OUTPUT 80772 transcriptions as txt files in 3-transcriptions/[diary_nid]/[page_nid].txt

3bis) DONE Download all images for each page
    INPUT: 2-PagesList-forEachDiary/[Diary-id].json
        -> run  python  3-Download-image-PerDiary-PerPage.py
        -> if page was not found: check for the default image, initialy set to "img/parts-of-the-leg.jpg"
    OUTPUT jpg files one per each page in 3-Transcriptions/[Diary-id]/[page-id].jpg

4) DONE I found API inconsistencies: 
    file        2-PagesList-forEachDiary/325182.json (an 93 other files)
    and file    2-PagesList-forEachDiary/182219.json (and 668 other files)
    have different JSON structure (!)
    -> added an "elif" for the two cases and empty diary detection
    -> Empty diaries detected: 115204.json 100411.json 115243.json 365805.json 100394.json 115202.json 338054.json 100393.json 100407.json 115213.json 115109.json 318442.json 115164.json 115175.json 100365.json 115212.json 281689.json 115162.json 318465.json 115245.json. 308755.json 100350.json 100403.json 325204.json  
    -> I remove all those entries
    -> Diaries now: 780
    -> API inconsitency inthis this file: http://transcripts.sl.nsw.gov.au/api/entity_node/272772

5) Parse diaries list and extend  diary metadata
    INPUT: "diaries-simple.json"
        run 5-diaries-title-parser.py > diaries.json
        Added fields: author', 'date_end', 'date_start', 'kind', 'nid', 'title', 'url'.
        PENDING of a manual review
    OUTPUT diaries.json
        -> manual edition of diaries.json. Tasks done:
            * Correct not well parsed data
            * As a first approach (later can be easily converted to general categories), I did chosse broad names for the propierty 'kind'.  For example: "letters from Gallipolli", instead of just "letters"
            * Format has been converted to correct JSON.
            * Notes on using the data:
                > if date_start is "":
                    if date_end is [only a year]:
                        date_start = date_end #put a year long diary 

6) DONE Topic model analysis with Mallet
    INPUT: all pages 3-TranscriptionsCLEAN/*
        > run mallet: check Mallet-commands.txt
        > Analysis done: 30, 50 and 100 topics. 
        > Human labelling has been tested for 30, 50 and 100 topics. 100 topics includes more and more specific topics. 
    OUTPUT: 6-Mallet-results/topics50:   crossreads_compostion.txt crossreads_keys.txt [converted to csv)

7) DONE Browser for topic models pages. Get n pages from topic model i, sort descendint or ascending.
    INPUT: 6-Mallet-results/topics50/crossreads_compostion.cvs'
        -> run 7-TopicMode-CSVbrowser.py [TM id] [Num pages] [desc/asc]
    OUTPUT (in console) text of [num pages] sorted by [TM id]. [desc/asc]/ This output is thrown to a files for each 
     TM, in version desc and asc. These will be precalculated files offered later in the interface. 

8) Check empty pages, and decide what to do 

9) DONE Review labelling using:
    INPUT: Mallet-results/topics100/crossreads_(  AND  7-TopicMode-CSVbrowser.py [TM id] [Num pages] [desc/asc] 
        * first labelling draft, supported by TopicMode-CSVbrowser
    OUTPUT: 6-Mallet-results/crossreads4_keys.cvs

9.1) Topic to keep/ to delete list:
    INPUT 6-Mallet-results/crossreads4_keys.cvs
        * Classify TMs in "to keep / to delete", supported by TopicMode-CSVbrowser
    OUTPUT: 6-Mallet-results/crossreads4_keys-classification.cvs

9.2) 5 topic groups w/ topics TO join list 
    INPUT: 6-Mallet-results/crossreads4_keys-2classification.cvs
        * Join topics list 
    OUTPUT: 6-Mallet-results/crossreads4_keys-3groups.cvs

9.3) 
    INPUT: join list of topics TO final labels.json
        - Join same-label TMs. And decide:
            * how to join scores
            * how to list top pages for each new TM
    OUTPUT: Create final topic models json file: labels.json

10) Topic models (TM) stats:
    - Number of pages with first TM / TM
    - Score average of first TM
    - Gauss (campana) for first TM score
    - 


11) Generate final files for each diary with pageID:[TM scores descending]
    9OR all together in one file for all diaries, as in crossreads version3?? I don't think so) 




-------------------------------
NOTE: These documents are in the collection but are not related to WWI-Diaries. These are not included in the visualization:

http://transcripts.sl.nsw.gov.au/api/entity_node/369991/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369992/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369993/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369994/?fields=nid,title,record_number,percentage_unlocked,transcript_pages
http://transcripts.sl.nsw.gov.au/api/entity_node/369995/?fields=nid,title,record_number,percentage_unlocked,transcript_pages

